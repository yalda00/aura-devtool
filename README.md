# AURA â€” Speak Code, Ship Everywhere

> *Vibe coding to the max.*

**AURA** is a voice-first, AI-powered developer tool that lets you code anywhere, anytime, without touching your computer. By turning spoken intent into real code through a real-time audio pipeline, AURA removes friction from development and keeps you in flow â€” even when youâ€™re away from your desk.

Built for developers who think better out loud, AURA enables hands-free coding, remote execution, and conversational feedback through a pure audio interface.

---

## ğŸš€ What AURA Does

- ğŸ™ Speak code into existence â€” no keyboard required  
- ğŸ“± Code from your phone or any audio-enabled device  
- âš¡ Get real-time feedback through voice  
- ğŸ§  Focus on intent, not syntax  
- ğŸŒ Code anywhere while AI works in the background  

AURA lets you go about your day while your ideas turn into real, executable code.

---

## ğŸ§  The Problem

Modern development workflows constantly interrupt flow.

- Commands take time to run  
- Errors appear late and break momentum  
- Developers are tied to screens and desks  
- Creativity is lost when ideas happen away from a laptop  

Even so-called â€œvibe codingâ€ today still involves too much waiting and sitting around.

---

## âœ¨ The Solution

AURA replaces screen-first development with a **voice-native workflow**.

Instead of typing commands or memorizing syntax, you talk. AURA listens, understands your intent using generative AI, applies code changes remotely, and responds with natural audio feedback â€” all in real time.

The result is faster iteration, fewer interruptions, and a more accessible way to build software.

---

## ğŸ— How It Works

AURA is powered by a real-time, voice-driven system:

1. User speaks into the **SwiftUI mobile app**
2. Audio is streamed via **LiveKit**
3. Speech is transcribed using **Speech-to-Text**
4. **Generative AI (Gemini)** interprets intent and generates code
5. Code is applied remotely through AI-assisted tooling
6. Responses are converted back to speech using **ElevenLabs**
7. Audio feedback closes the loop in real time

This creates a fully conversational, hands-free coding experience.

---

## ğŸ›  Technologies Used

- **SwiftUI** â€” mobile UI  
- **LiveKit** â€” real-time audio streaming  
- **ElevenLabs** â€” text-to-speech  
- **Gemini API** â€” intent understanding & code generation  
- **Trae** â€” AI-native development environment  
- **Claude** â€” agentic coding support  
- **Node.js** â€” backend orchestration  
- **Ngrok** â€” secure tunneling for remote access  

---

## ğŸ† Accomplishments

- Built a fully working real-time voice â†’ code pipeline  
- Achieved low-latency, conversational audio feedback  
- Enabled hands-free coding without a screen  
- Designed for productivity, creativity, and accessibility  

---

## ğŸ“š What We Learned

- Real-time audio systems are highly sensitive to latency  
- Voice-first interfaces must be designed around intent, not commands  
- Small interruptions have a huge impact on developer flow  
- Generative AI becomes far more powerful when paired with natural interaction  

---

## ğŸ”® Whatâ€™s Next

- Deeper integration with autonomous coding agents  
- GitHub Copilotâ€“style repo-level control  
- Hardware extensions (e.g. ESP32 earbuds) for device-free coding  
- Improved collaboration and context awareness  

---

## ğŸ‘¥ Team

Built by:
- Stephanie Wan  
- Yalda Nikookar  
- Bowen Zhu  

---

**AURA**  
*Speak code. Ship everywhere.*
